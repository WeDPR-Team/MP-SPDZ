#This file is auto generated by ppc. DO NOT EDIT!

#PSI_OPTION=False
from ppc import *
from Compiler import config
import sys
program.options_from_args()

SOURCE0=0
SOURCE1=1
SOURCE2=2
# source0_record_count=$(source0_record_count)
# source1_record_count=$(source1_record_count)
# source2_record_count=$(source2_record_count)
# total_record_count=source0_record_count+source1_record_count+source2_record_count
# train_record_count=$(train_record_count)
# test_record_count=$(test_record_count)

source0_record_count=200
source1_record_count=250
source2_record_count=119
total_record_count=source0_record_count+source1_record_count+source2_record_count
train_record_count=429
test_record_count= total_record_count - train_record_count

# source0_record_count=2
# source1_record_count=2
# source2_record_count=2
# total_record_count=source0_record_count+source1_record_count+source2_record_count
# train_record_count=4
# test_record_count=2

epochs=1
batch_size=train_record_count
threads=8

# sfix.set_precision(32, 63)
# cfix.set_precision(32, 63)

# total_feature_count=$(total_feature_count)
total_feature_count=30
dataset_Y=pfix.Array(total_record_count)
dataset_X=pfix.Matrix(total_record_count, total_feature_count)

def read_homo_dataset():
	for i in range(total_record_count):
		if i < source0_record_count:
			for j in range(total_feature_count + 1):
				if j == 0:
					dataset_Y[i] = pfix.get_input_from(SOURCE0)
				else:
					dataset_X[i][j - 1] = pfix.get_input_from(SOURCE0)
		if i >= source0_record_count and i < source0_record_count + source1_record_count:
			for j in range(total_feature_count + 1):
				if j == 0:
					dataset_Y[i] = pfix.get_input_from(SOURCE1)
				else:
					dataset_X[i][j - 1] = pfix.get_input_from(SOURCE1)
		if i >= source0_record_count + source1_record_count and i < source0_record_count + source1_record_count + source2_record_count:
			for j in range(total_feature_count + 1):
				if j == 0:
					dataset_Y[i] = pfix.get_input_from(SOURCE2)
				else:
					dataset_X[i][j - 1] = pfix.get_input_from(SOURCE2)

layers = [pDense(train_record_count, total_feature_count, 1), ml.Output(train_record_count, approx=3)]

train_Y = pfix.Array(train_record_count)
train_X = pfix.Matrix(train_record_count, total_feature_count)
test_Y = pfix.Array(test_record_count)
test_X = pfix.Matrix(test_record_count, total_feature_count)


def get_train_test_dataset():
    for i in range(train_record_count):
        train_Y[i] = dataset_Y[i]
        train_X[i] = dataset_X[i]

    for i in range(train_record_count, total_record_count):
        test_Y[i-train_record_count] = dataset_Y[i]
        test_X[i-train_record_count] = dataset_X[i]


def safe_store(mem_address, item, item_size):
    if mem_address + item_size >= config.USER_MEM:
        sys.exit("Out of Memory")
    item.store_in_mem(mem_address)
    return mem_address + item_size


def safe_store_collection(mem_address, item):
    return safe_store(mem_address, item.get_vector(), item.total_size())


def safe_store_layers(mem_address, layers):
    for layer in layers:
        if isinstance(layer, ml.ConvBase):
            print_ln("Saving ConvBase")
            mem_address = safe_store_collection(mem_address, layer.weights)
            mem_address = safe_store_collection(mem_address, layer.bias)
        elif isinstance(layer, ppcDense):
            print_ln("Saving Dense")
            mem_address = safe_store_collection(mem_address, layer.W)
            mem_address = safe_store_collection(mem_address, layer.b)
        elif isinstance(layer, ml.NoVariableLayer) or isinstance(layer, ml.ElementWiseLayer):
            pass
        else:
            print_ln("Unknown layer type: %s", type(layer))
            assert False


def run_train():
    ml.Layer.threads = threads
    global layers
    layers[0].X.assign(train_X)
    layers[-1].Y.assign(train_Y)

    sgd = ml.SGD(layers, epochs, report_loss=True)
    sgd.reset()

    start_timer(1)
    sgd.run(batch_size)
    stop_timer(1)

    # Save the weights.
    mem_address = 0
    safe_store_layers(mem_address, layers)

    # Run the test set
    layers[-1].approx = False
    test_pred = sgd.eval(test_X)

    print_ln()
    # if isinstance(test_pred, Matrix):
    #     class_count = len(test_pred[0])
    #     print_str("result_fields = class_label ")
    #     for j in range(class_count):
    #         print_str("class%s_prob ", j)
    #     print_ln()
    # else:
    #     print_ln("result_fields = class_label class_prob")
    # for i in range(test_record_count):
    #     print_str("result_values = ")
    #     print_str('%s ', test_Y[i].reveal())
    #     if isinstance(test_pred, Matrix):
    #         class_count = len(test_pred[0])
    #         for j in range(class_count):
    #             print_str('%s ', test_pred[i][j].reveal())
    #     else:
    #         print_str('%s', test_pred[i].reveal())
    #     print_ln()

    printfmt("y_true_arr ")
    for i in range(test_record_count):
        printfmt('%s,', test_Y[i].reveal())
    println()

    printfmt("y_scores_arr ")
    for i in range(test_record_count):
        printfmt('%s,', test_pred[i].reveal())
    println()

read_homo_dataset()
get_train_test_dataset()
run_train()

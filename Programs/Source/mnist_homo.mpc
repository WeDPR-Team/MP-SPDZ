#This file is auto generated by ppc. DO NOT EDIT!

#PSI_OPTION=False
from ppc import *
from Compiler import config
import sys
program.options_from_args()

SOURCE0=0
SOURCE1=1
SOURCE2=2
source0_record_count=1000
source1_record_count=1000
source2_record_count=1000
total_record_count=source0_record_count+source1_record_count+source2_record_count
train_record_count=2500
test_record_count=total_record_count - train_record_count

# source0_record_count=3000
# source1_record_count=4000
# source2_record_count=3000
# total_record_count=source0_record_count+source1_record_count+source2_record_count
# train_record_count=9500
# test_record_count=total_record_count - train_record_count

# source0_record_count=20
# source1_record_count=30
# source2_record_count=10
# total_record_count=source0_record_count+source1_record_count+source2_record_count
# train_record_count=45
# test_record_count=total_record_count - train_record_count

epochs=1
batch_size=min(128, train_record_count)
threads=8

# sfix.set_precision(32, 63)
# cfix.set_precision(32, 63)

total_feature_count=784
total_class_count=10
# dataset_Y=pint.Array(total_record_count)
# dataset_Y=pint.Matrix(total_record_count, total_class_count)
# dataset_X=pfix.Matrix(total_record_count, total_feature_count)

layers = [pDense(train_record_count, total_feature_count, 128, activation='relu'),
pDense(train_record_count, 128, 128, activation='relu'),
pDense(train_record_count, 128, 10),
ml.MultiOutput(train_record_count, 10)]

# layers = [pDense(train_record_count, total_feature_count, 10, activation='relu'),
# ml.MultiOutput(train_record_count, 10)]

# train_Y = pint.Matrix(train_record_count, total_class_count)
# train_X = pfix.Matrix(train_record_count, total_feature_count)
test_Y = pint.Matrix(test_record_count, total_class_count)
test_X = pfix.Matrix(test_record_count, total_feature_count)

def read_homo_dataset():
    train_X = layers[0].X
    train_Y = layers[-1].Y

    # print(train_X.sizes, train_Y.sizes)

    def read_y(i, j, party):
        if i < train_record_count:
            train_Y[i][j] = pint.get_input_from(party)
        else:
            test_Y[i - train_record_count][j] = pint.get_input_from(party)

    def read_x(i, j, party):
        if i < train_record_count:
            train_X[i][0][j - total_class_count] = pfix.get_input_from(party)
        else:
            test_X[i - train_record_count][j - total_class_count] = pfix.get_input_from(party)

    for i in range(total_record_count):
        if i < source0_record_count:
            for j in range(total_feature_count + total_class_count):
                if j < total_class_count:
                    read_y(i, j, SOURCE0)
                    # dataset_Y[i][j] = pint.get_input_from(SOURCE0)
                else:
                    read_x(i, j, SOURCE0)
                    # dataset_X[i][j - total_class_count] = pfix.get_input_from(SOURCE0)
        if i >= source0_record_count and i < source0_record_count + source1_record_count:
            for j in range(total_feature_count + total_class_count):
                if j < total_class_count:
                    read_y(i, j, SOURCE1)
                    # dataset_Y[i][j] = pint.get_input_from(SOURCE1)
                else:
                    read_x(i, j, SOURCE1)
                    # dataset_X[i][j - total_class_count] = pfix.get_input_from(SOURCE1)
        if i >= source0_record_count + source1_record_count and i < source0_record_count + source1_record_count + source2_record_count:
            for j in range(total_feature_count + total_class_count):
                if j < total_class_count:
                    read_y(i, j, SOURCE2)
                    # dataset_Y[i][j] = pint.get_input_from(SOURCE2)
                else:
                    read_x(i, j, SOURCE2)
                    # dataset_X[i][j - total_class_count] = pfix.get_input_from(SOURCE2)


# def get_train_test_dataset():
#     for i in range(train_record_count):
#         train_Y[i] = dataset_Y[i]
#         train_X[i] = dataset_X[i]

#     for i in range(train_record_count, total_record_count):
#         test_Y[i-train_record_count] = dataset_Y[i]
#         test_X[i-train_record_count] = dataset_X[i]


def safe_store(mem_address, item, item_size):
    if mem_address + item_size >= config.USER_MEM:
        sys.exit("Out of Memory")
    item.store_in_mem(mem_address)
    return mem_address + item_size


def safe_store_collection(mem_address, item):
    print('csize= ', item.total_size())
    return safe_store(mem_address, item.get_vector(), item.total_size())


def safe_store_layers(mem_address, layers):
    for layer in layers:
        if isinstance(layer, ml.ConvBase):
            print_ln("Saving ConvBase")
            mem_address = safe_store_collection(mem_address, layer.weights)
            mem_address = safe_store_collection(mem_address, layer.bias)
        elif isinstance(layer, ppcDense):
            print_ln("Saving Dense")
            mem_address = safe_store_collection(mem_address, layer.W)
            mem_address = safe_store_collection(mem_address, layer.b)
        elif isinstance(layer, ml.NoVariableLayer) or isinstance(layer, ml.ElementWiseLayer):
            pass
        else:
            print_ln("Unknown layer type: %s", type(layer))
            assert False


def run_train():
    ml.Layer.threads = threads
    global layers
    # layers[0].X.assign(train_X)
    # layers[-1].Y.assign(train_Y)

    # print_matrix(train_X)
    # print_matrix(train_Y)

    sgd = ml.SGD(layers, epochs, report_loss=True)
    sgd.reset()

    start_timer(1)
    sgd.gamma = MemValue(cfix(.1))
    sgd.run(batch_size)
    stop_timer(1)

    # Save the weights.
    # mem_address = 0
    # safe_store_layers(mem_address, layers)

    # Run the test set
    # print(test_X.sizes)
    # layers[-1].approx = False
    # test_pred = sgd.eval(test_X)
    sgd.layers[0].X.assign(test_X)
    sgd.forward(test_record_count)

    # print_matrix(test_X)
    # print_matrix(test_Y)

    print_ln()
    # if isinstance(test_pred, Matrix):
    #     class_count = len(test_pred[0])
    #     print_str("result_fields = class_label ")
    #     for j in range(class_count):
    #         print_str("class%s_prob ", j)
    #     print_ln()
    # else:
    #     print_ln("result_fields = class_label class_prob")
    # for i in range(test_record_count):
    #     print_str("result_values = ")
    #     print_str('%s ', test_Y[i].reveal())
    #     if isinstance(test_pred, Matrix):
    #         class_count = len(test_pred[0])
    #         for j in range(class_count):
    #             print_str('%s ', test_pred[i][j].reveal())
    #     else:
    #         print_str('%s', test_pred[i].reveal())
    #     print_ln()

    printfmt("y_label ")
    for i in range(test_record_count):
        printfmt('%s,', test_Y[i].reveal())
    println()

    printfmt("y_pred ")
    for i in range(test_record_count):
        printfmt('%s,', sgd.layers[-1].X[i].reveal())
    println()
    # class_count = len(test_pred[0])
    # for i in range(test_record_count):
    #     printfmt("[")
    #     for j in range(class_count):
    #         printfmt('%s,', test_pred[i][j].reveal())
    #     printfmt("],")
    # println()

read_homo_dataset()
# get_train_test_dataset()
run_train()
